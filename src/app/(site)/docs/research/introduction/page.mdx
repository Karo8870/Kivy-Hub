# Introduction

## Motivation

Food is a central aspect of human life, impacting not only health and well-being but also culture, identity, and sustainability. As digital platforms increasingly become the primary source of culinary information, there is a growing need to build intelligent systems that can understand, organize, and personalize recipe content. However, recipes are inherently complex: they combine structured and unstructured data, rely heavily on domain-specific language, and often lack standardization. Learning meaningful representations or embeddings of recipes can enable machines to reason about food in a way that is both flexible and scalable. Such embeddings have wide-ranging applications: they can power personalized recipe recommendation engines, facilitate ingredient-based search, support dietary and allergy-aware meal planning, or even serve as the foundation for generative models that create new dishes. Despite these possibilities, existing recipe datasets are often noisy, outdated, or lack the granularity required for fine-grained modeling. Furthermore, current embedding methods typically rely on large multimodal setups, which may be impractical for lightweight or text-only use cases. To address these challenges, we introduce a new curated dataset from Food.com and propose an autoencoder-based architecture that learns latent recipe representations from purely textual inputs. Our goal is to provide both high-quality data and a flexible, eﬃcient model architecture that can serve as a foundation for the next generation of food-aware applications.

## Problem Statement

Despite the growing interest in computational food understanding, there remains a lack of high-quality, structured datasets and eﬃcient, general-purpose models for learning robust representations of textual recipe data. Existing approaches to recipe embedding often depend on multimodal data (e.g., images, user interactions) or rely on shallow textual features that fail to capture the deeper semantic relationships between ingredients, instructions, and culinary intent. Moreover, many available datasets contain noisy or incomplete information, such as improperly formatted ingredient lists, ambiguous instructions, or inconsistent metadata. This significantly limits the ability of models to generalize across tasks such as retrieval, classification, or recipe generation. Without clean data and strong embeddings, intelligent food systems are constrained in their ability to oﬀer accurate recommendations, enable creative substitutions, or support health-aware personalization. To bridge this gap, there is a need for (1) a reliable, well-structured textual recipe dataset, and (2) a model capable of encoding recipes into dense, meaningful representations that capture their functional, cultural, and nutritional context. This paper addresses both challenges.