# Abstract

Understanding and representing culinary recipes in a meaningful and machine-interpretable way is a growing area of interest in food computing, with applications in recommendation systems, dietary planning, and generative cooking models. In this paper, we present two major contributions to the field. First, we release a newly curated and cleaned dataset of over 500k recipes scraped from Food.com, containing structured ingredient lists, cooking instructions, titles, and metadata such as cuisine and dietary tags. This dataset oﬀers a high-quality, diverse, and reproducible benchmark for future research. Second, we propose a novel autoencoder-based architecture designed to learn robust recipe embeddings that capture semantic, structural, and functional aspects of recipes. By training the model to reconstruct full recipes from latent representations, the autoencoder learns embeddings that are compact yet informative. We demonstrate the eﬀectiveness of these embeddings across several downstream tasks, including ingredient-based recipe retrieval, cuisine classification, and recipe similarity search, achieving competitive results without relying on multimodal inputs. Our findings suggest that autoencoder-based embeddings can serve as a strong foundation for a wide range of intelligent culinary applications, and we provide our code and dataset publicly to support further exploration in this domain.