# Abstract

Understanding and representing culinary recipes in a meaningful and machine-interpretable way is a growing area of interest in food computing, with applications in recommendation systems, dietary planning, and generative cooking models. In this paper, we present two major contributions to the field.

First, we introduce a newly curated and cleaned dataset of over 500,000 recipes sourced from Food.com, containing structured ingredient lists, step-by-step instructions, titles, and metadata such as cuisine and dietary tags. This dataset provides a high-quality, diverse, and reproducible benchmark for future research in computational gastronomy.

Second, we propose a novel autoencoder-based architecture that learns robust recipe embeddings, capturing the semantic, structural, and functional aspects of culinary data. By reconstructing full recipes from latent representations, our model generates compact yet informative embeddings. We demonstrate the effectiveness of these embeddings across multiple downstream tasks, including ingredient-based retrieval, cuisine classification, and recipe similarity search - achieving strong results without relying on multimodal inputs.

Our findings suggest that autoencoder-based embeddings can serve as a strong foundation for a wide range of intelligent culinary applications, and we provide our code and dataset publicly to support further exploration in this domain.
